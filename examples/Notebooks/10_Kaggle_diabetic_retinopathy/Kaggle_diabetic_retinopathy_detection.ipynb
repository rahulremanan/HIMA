{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Diabetic_retinopathy_detection.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "n8Ke0jZXVI72",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# [Diabetic retinopathy detection](https://www.kaggle.com/c/diabetic-retinopathy-detection/)\n",
        "\n",
        "## [An overview of the diabetic retinopathy](https://nei.nih.gov/health/diabetic/retinopathy)\n",
        "Diabetic retinopathy is a complication of diabetes affects the retinas of the eyes. The retinas are the part of the eye that detects ligh and converts into nerve signals that are then conveyed to the visual cortex in the brain, via the optic nerve. It is the most common cause of vision loss among individuals with diabetes.\n",
        "\n",
        "It's caused by damage to the blood vessels of the light-sensitive tissue at the back of the eye (retina), due to the leaking of fluid from the retinal blood vessels or due to a due to a hemorrhage (bleeding) from these blood vessels. At first, diabetic retinopathy may cause no symptoms or only mild vision problems.\n",
        "\n",
        "## Creating a deep neural network that detects the presence of diabetic retinopathy:\n",
        "\n",
        "The dataset is a large set of high-resolution retina images taken under a variety of imaging conditions. A left and right field is provided for every subject. Images are labeled with a subject id as well as either left or right (e.g. 1_left.jpeg is the left eye of patient id 1).\n",
        "\n",
        "A clinician has rated the presence of diabetic retinopathy in each image on a scale of 0 to 4.\n",
        "\n",
        "This staging has direct relevance to progression of the disease.\n",
        "\n",
        "Using these images and corresponding labels a deep convolutional neural network can be trained to detect retinal images for features of diabetic retinopathy.\n",
        "\n",
        "## An overview of the diabetic retinopathy staging:\n",
        "\n",
        "**1) Mild nonproliferative retinopathy:** Small areas of balloon-like swelling in the retina’s tiny blood vessels, called microaneurysms, occur at this earliest stage of the disease. These microaneurysms may leak fluid into the retina.\n",
        "\n",
        "**2) Moderate nonproliferative retinopathy:** As the disease progresses, blood vessels that nourish the retina may swell and distort. They may also lose their ability to transport blood. Both conditions cause characteristic changes to the appearance of the retina and may contribute to DME.\n",
        "\n",
        "**3) Severe nonproliferative retinopathy:** Many more blood vessels are blocked, depriving blood supply to areas of the retina. These areas secrete growth factors that signal the retina to grow new blood vessels.\n",
        "\n",
        "**4) Proliferative diabetic retinopathy (PDR):** At this advanced stage, growth factors secreted by the retina trigger the proliferation of new blood vessels, which grow along the inside surface of the retina and into the vitreous gel, the fluid that fills the eye. The new blood vessels are fragile, which makes them more likely to leak and bleed. Accompanying scar tissue can contract and cause retinal detachment—the pulling away of the retina from underlying tissue, like wallpaper peeling away from a wall. Retinal detachment can lead to permanent vision loss.\n",
        "\n",
        "## About the dataset:\n",
        "\n",
        "The images in the dataset come from different models and types of cameras, which can affect the visual appearance of left vs. right. Some images are shown as one would see the retina anatomically (macula on the left, optic nerve on the right for the right eye). Others are shown as one would see through a microscope condensing lens (i.e. inverted, as one sees in a typical live eye exam). There are generally two ways to tell if an image is inverted:\n",
        "\n",
        "It is inverted if the macula (the small dark central area) is slightly higher than the midline through the optic nerve. If the macula is lower than the midline of the optic nerve, it's not inverted.\n",
        "If there is a notch on the side of the image (square, triangle, or circle) then it's not inverted. If there is no notch, it's inverted.\n",
        "\n",
        "Like any real-world data set, you will encounter noise in both the images and labels. Images may contain artifacts, be out of focus, underexposed, or overexposed. A major aim of this competition is to develop robust algorithms that can function in the presence of noise and variation.\n",
        "\n",
        "## Data labels:\n",
        "* 0 - No DR\n",
        "* 1 - Mild\n",
        "* 2 - Moderate\n",
        "* 3 - Severe\n",
        "* 4 - Proliferative DR"
      ]
    },
    {
      "metadata": {
        "id": "fiwxfDJFcUjo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "setup = True\n",
        "fetch_raw_data = False\n",
        "colab_mode = True\n",
        "upload_data = False\n",
        "download_data = True\n",
        "verbose = False\n",
        "\n",
        "dataset_id = 'kaggle_diabetic_retinopathy'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KLYK4C9QUvcx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os \n",
        "import sys\n",
        "import subprocess\n",
        "import gc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lB5A1DPsbzc1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def execute_in_shell(command=None, \n",
        "                     verbose = False):\n",
        "    \"\"\" \n",
        "        command -- keyword argument, takes a list as input\n",
        "        verbsoe -- keyword argument, takes a boolean value as input\n",
        "    \n",
        "        This is a function that executes shell scripts from within python.\n",
        "        \n",
        "        Keyword argument 'command', should be a list of shell commands.\n",
        "        Keyword argument 'verbose', should be a boolean value to set verbose level.\n",
        "        \n",
        "        Example usage: execute_in_shell(command = ['ls ./some/folder/',\n",
        "                                                    ls ./some/folder/  -1 | wc -l'],\n",
        "                                        verbose = True ) \n",
        "                                        \n",
        "        This command returns dictionary with elements: Output and Error.\n",
        "        \n",
        "        Output records the console output,\n",
        "        Error records the console error messages.\n",
        "                                        \n",
        "    \"\"\"\n",
        "    error = []\n",
        "    output = []\n",
        "    \n",
        "    if isinstance(command, list):\n",
        "        for i in range(len(command)):\n",
        "            try:\n",
        "                process = subprocess.Popen(command[i], shell=True, stdout=subprocess.PIPE)\n",
        "                process.wait()\n",
        "                out, err = process.communicate()\n",
        "                error.append(err)\n",
        "                output.append(out)\n",
        "                if verbose:\n",
        "                    print ('Success running shell command: {}'.format(command[i]))\n",
        "            except Exception as e:\n",
        "                print ('Failed running shell command: {}'.format(command[i]))\n",
        "                if verbose:\n",
        "                    print(type(e))\n",
        "                    print(e.args)\n",
        "                    print(e)\n",
        "                \n",
        "    else:\n",
        "        print ('The argument command takes a list input ...')\n",
        "    return {'Output': output, 'Error': error }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pSLQvMF3b390",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "command = ['pip3 install -q kaggle PyDrive scikit-optimize >/dev/null 2>&1',\n",
        "           'mkdir ~/.kaggle/',\n",
        "           'mkdir ./{}/'.format(dataset_id)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yQwWaoxOb6D7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "71fc8b71-1442-439f-b10c-b79b8686f913"
      },
      "cell_type": "code",
      "source": [
        "if setup and colab_mode:\n",
        "  execute_in_shell(command = command, \n",
        "                   verbose = True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success running shell command: pip3 install -q kaggle PyDrive scikit-optimize >/dev/null 2>&1\n",
            "Success running shell command: mkdir ~/.kaggle/\n",
            "Success running shell command: mkdir ./kaggle_diabetic_retinopathy/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZlS4HcEgb-_l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if colab_mode:\n",
        "    from pydrive.auth import GoogleAuth\n",
        "    from pydrive.drive import GoogleDrive\n",
        "    from google.colab import auth\n",
        "    from oauth2client.client import GoogleCredentials\n",
        "    from googleapiclient.http import MediaIoBaseDownload\n",
        "    \n",
        "import io\n",
        "import glob\n",
        "import fnmatch\n",
        "import random\n",
        "\n",
        "from multiprocessing import Process\n",
        "\n",
        "import os, sys, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from imgaug import augmenters as iaa\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0_rsKizccA6B",
        "colab_type": "code",
        "outputId": "3dae971e-049d-40f6-8415-e74a664a49a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import sys\n",
        "import glob\n",
        "try:\n",
        "    import h5py\n",
        "except:\n",
        "    print ('Package h5py needed for saving model weights ...')\n",
        "    sys.exit(1)\n",
        "import json\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "try:\n",
        "    import tensorflow\n",
        "    import keras\n",
        "except:\n",
        "    print ('This code uses tensorflow deep-learning framework and keras api ...')\n",
        "    print ('Install tensorflow and keras to train the classifier ...')\n",
        "    sys.exit(1)\n",
        "import PIL\n",
        "from collections import defaultdict\n",
        "from keras.applications.inception_v3 import InceptionV3,    \\\n",
        "                                            preprocess_input as preprocess_input_inceptionv3\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2,    \\\n",
        "                                            preprocess_input as preprocess_input_inceptionv4\n",
        "from keras.models import Model,                             \\\n",
        "                         model_from_json,                    \\\n",
        "                         load_model\n",
        "from keras.layers import Dense,                             \\\n",
        "                         GlobalAveragePooling2D,            \\\n",
        "                         Dropout,                           \\\n",
        "                         BatchNormalization\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import SGD,                           \\\n",
        "                             RMSprop,                       \\\n",
        "                             Adagrad,                       \\\n",
        "                             Adadelta,                      \\\n",
        "                             Adam,                          \\\n",
        "                             Adamax,                        \\\n",
        "                             Nadam\n",
        "from keras.callbacks import EarlyStopping,   \\\n",
        "                            ModelCheckpoint, \\\n",
        "                            ReduceLROnPlateau\n",
        "                            \n",
        "from multiprocessing import Process"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "RBi8IFFld3ks",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Authenticate notebook session to access Kaggle\n",
        "\n",
        "The authentication of the notebook session in CoLab can be done via the Kaggle API. Download the kaggle.json file to your computer, from [Kaggle account settings](https://www.kaggle.com/{USERNAME}/account) under API, using the **\"Create New API Token\"** button. \n",
        "\n",
        "Upload that kaggle.json file to this session by executing the cell below."
      ]
    },
    {
      "metadata": {
        "id": "z4wIb6Z2cEOC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if setup and fetch_raw_data and colab_mode:\n",
        "  from google.colab import files\n",
        "  uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VavIh3pCgq-l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download the data\n",
        "\n",
        "Using the Kaggle authentication token, the retinal imaging data will be downloaded, via the [Kaggle API](https://github.com/Kaggle/kaggle-api).\n",
        "\n",
        "`kaggle competitions download -c diabetic-retinopathy-detection`\n",
        "\n",
        "Before downloading the data, ensure that you have accepted the [Kaggle platform rules for the diabetic retinopathy detection competition](https://www.kaggle.com/c/diabetic-retinopathy-detection/rules)."
      ]
    },
    {
      "metadata": {
        "id": "qJKmxOxHdJLv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "command = ['mkdir ~/.kaggle/',\n",
        "           'mv ./kaggle.json /root/.kaggle/',\n",
        "           'chmod 600 ~/.kaggle/kaggle.json',\n",
        "           'kaggle competitions download -c diabetic-retinopathy-detection']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zKaUAbnAfDSw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if fetch_raw_data:\n",
        "  execute_in_shell(command = command, verbose = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FqjWo7hQuMnG",
        "colab_type": "code",
        "outputId": "dd6fc001-9263-4b3b-bee6-e35b3a7c211e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "! ls "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle_diabetic_retinopathy  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b-tJxBr1oP96",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "command = ['mv ./train.zip.001 ./train_{}.zip.001'.format(dataset_id),\n",
        "           'mv ./train.zip.002 ./train_{}.zip.002'.format(dataset_id),\n",
        "           'mv ./train.zip.003 ./train_{}.zip.003'.format(dataset_id),\n",
        "           'mv ./train.zip.004 ./train_{}.zip.004'.format(dataset_id),\n",
        "           'mv ./train.zip.005 ./train_{}.zip.005'.format(dataset_id),\n",
        "           'mv ./trainLabels.csv.zip ./trainLabels_{}.csv.zip'.format(dataset_id),\n",
        "           'mv ./test.zip.001 ./test_{}.zip.001'.format(dataset_id),\n",
        "           'mv ./test.zip.002 ./test_{}.zip.002'.format(dataset_id),\n",
        "           'mv ./test.zip.003 ./test_{}.zip.003'.format(dataset_id),\n",
        "           'mv ./test.zip.004 ./test_{}.zip.004'.format(dataset_id),\n",
        "           'mv ./test.zip.005 ./test_{}.zip.005'.format(dataset_id),\n",
        "           'mv ./test.zip.006 ./test_{}.zip.006'.format(dataset_id),\n",
        "           'mv ./test.zip.007 ./test_{}.zip.007'.format(dataset_id),\n",
        "           'mv ./sample_submission.csv.zip ./sample_submission_{}'.format(dataset_id)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HAtw-PpBvaVv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if fetch_raw_data:\n",
        "  execute_in_shell(command = command, verbose = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ssDlOVLqwuvA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cloud_authenticate():\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "  print (\"Sucessfully authenticated to access Google Drive ...\")\n",
        "  return drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rVCI1ARCwxkt",
        "colab_type": "code",
        "outputId": "9945dfb4-ada5-4296-c0bd-34d8abf03fc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "if colab_mode:\n",
        "    drive = cloud_authenticate()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sucessfully authenticated to access Google Drive ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NykTfYd5w91p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def googledrive_fetch(file_name = None, \n",
        "                fetch=True, \n",
        "                fetch_by_id = False,\n",
        "                latest = True,\n",
        "                file_id = None,\n",
        "                multi_file = False):\n",
        "  \n",
        "  \"\"\"\n",
        "    A function that fetches files from Google Drive.\n",
        "    \n",
        "    The function takes five keyword arguments:\n",
        "      file_name -- Passes the file name string\n",
        "      fetch -- Specify if a file name should be downloaded\n",
        "      fetch_by_id -- Specify a file to be downloaded by file id\n",
        "      multi_file -- Download all the files with the same file name from Google Drive\n",
        "  \"\"\"\n",
        "  \n",
        "  query = 'title='+\"'\"+file_name+\"'\"\n",
        "  try:\n",
        "    file_list=drive.ListFile({'q': \"{}\".format(query)}).GetList()\n",
        "  except:\n",
        "    return (\"Error finding file with {}\".format(query))\n",
        "  \n",
        "  if len(file_list) >1:\n",
        "    print (\"A total of {} files with the same file name found ...\".format(len(file_list)))\n",
        "    for f in file_list:\n",
        "      title = f['title']\n",
        "      id = f.metadata.get('id')\n",
        "      print (\"Found: {} file, with file id: {}\".format(title, id))\n",
        "    \n",
        "    if multi_file:\n",
        "      print (\"Downloading {} files with file name {}\".format(len(file_list), title))\n",
        "      print (\"Staring download ...\")\n",
        "    elif latest:\n",
        "      print (\"Downloading the most recent {} file ...\".format(title))\n",
        "    elif file_id == None:\n",
        "      print (\"Set keyword argument fetch_by_id = True and specify id using keyword argument file_id = 'id' to download a specific file ...\")\n",
        "      print (\"--OR--\")\n",
        "      print (\"Set keyword argument multi_file = True to automatically download all the files ...\")\n",
        "      return None\n",
        "    else:\n",
        "      print (\"Starting download ...\")\n",
        "    \n",
        "  n = 0\n",
        "  \n",
        "  if latest:\n",
        "    try:\n",
        "      title = file_list[0]['title']\n",
        "    except:\n",
        "      return (\"Error finding file with {}\".format(query))\n",
        "    latest_file_id = file_list[0].metadata.get('id')\n",
        "    print (\"Found most recent version of: {} file with file id: {} ...\".format(title, latest_file_id))    \n",
        "  \n",
        "  for f in file_list:\n",
        "      if fetch and multi_file and n>0:\n",
        "        save_path = os.path.join('./'+str(n)+'_'+file_name)\n",
        "      else:\n",
        "        save_path = os.path.join('./'+file_name)     \n",
        "      \n",
        "      title = f['title']\n",
        "      \n",
        "      if fetch_by_id and file_id !=None:\n",
        "        id = file_id\n",
        "      elif latest:\n",
        "        id = latest_file_id\n",
        "      elif fetch_by_id and file_id == None:\n",
        "        print ('Please specify the file id for downloading using the file_id argument ...')\n",
        "      else:\n",
        "        id = f.metadata.get('id')\n",
        "      \n",
        "      print (\"Downloading {} file, with file id: {} ...\".format(title, id))\n",
        "      \n",
        "      if fetch or fetch_by_id or latest:\n",
        "        local_file = io.FileIO(save_path, mode='wb')\n",
        "        try:\n",
        "          request = drive.auth.service.files().get_media(fileId=id)\n",
        "          downloader = MediaIoBaseDownload(local_file, request, chunksize=2048*102400)\n",
        "\n",
        "          done = False\n",
        "\n",
        "          while done is False:\n",
        "              status, done = downloader.next_chunk()\n",
        "        except:\n",
        "          return 'Downloading failed ...'\n",
        "        \n",
        "        local_file.close()\n",
        "        print (\"Successfully downloaded the file: {} to: {} ...\".format(file_name, save_path))\n",
        "      \n",
        "      if fetch_by_id and file_id !=None:\n",
        "        return None\n",
        "      elif latest:\n",
        "        return None\n",
        "      elif n >= 0:\n",
        "        print (\"Downloaded {} of {} files ...\".format(n+1, len(file_list)))\n",
        "      else:\n",
        "        print (\"Download failed ...\")\n",
        "      \n",
        "      n +=1\n",
        "  \n",
        "  return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mhVMRTPkw-FZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def googledrive_save(file_name = None, \n",
        "               file_dir = None, \n",
        "               upload = False,\n",
        "               prefix = None):\n",
        "  if upload == True and file_name != None and file_dir !=None:\n",
        "    try:\n",
        "      if prefix != None:\n",
        "        file = drive.CreateFile({'title': str(prefix) + str(file_name) })\n",
        "      else:\n",
        "        file = drive.CreateFile({'title': str(file_name) })\n",
        "      file.SetContentFile(os.path.join(file_dir + str(file_name)))\n",
        "      file.Upload()\n",
        "      print (str(file_name) + \" successfully uploaded to Google drive ...\")\n",
        "    except:\n",
        "      print (\"Failed to save :\" + str(file_name) + \" to Google drive ...\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OqXCJIsVvc8Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file_dir = './'\n",
        "file_name = ['test_{}.zip.001'.format(dataset_id),\n",
        "             'test_{}.zip.002'.format(dataset_id),\n",
        "             'test_{}.zip.003'.format(dataset_id),\n",
        "             'test_{}.zip.004'.format(dataset_id),\n",
        "             'test_{}.zip.005'.format(dataset_id),\n",
        "             'test_{}.zip.006'.format(dataset_id),\n",
        "             'test_{}.zip.007'.format(dataset_id),\n",
        "             'train_{}.zip.001'.format(dataset_id),\n",
        "             'train_{}.zip.002'.format(dataset_id),\n",
        "             'train_{}.zip.003'.format(dataset_id),\n",
        "             'train_{}.zip.004'.format(dataset_id),\n",
        "             'train_{}.zip.005'.format(dataset_id),\n",
        "             'trainLabels_{}.csv.zip'.format(dataset_id)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JcJY_rDxxgMh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Upload the dataset to Google drive"
      ]
    },
    {
      "metadata": {
        "id": "C9BtmHcuxD5j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if upload_data and colab_mode:\n",
        "  for f in file_name:\n",
        "    googledrive_save(file_name = f,\n",
        "                     file_dir = file_dir,\n",
        "                     upload = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ejq6ZKxPAIvl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "71acb6df-2f2e-4870-eda2-156c7592490f"
      },
      "cell_type": "code",
      "source": [
        "if download_data and colab_mode:\n",
        "  for f in file_name:\n",
        "    googledrive_fetch(file_name = f, \n",
        "                      fetch=True, \n",
        "                      latest = True)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found most recent version of: test_kaggle_diabetic_retinopathy.zip.001 file with file id: 1ixiYl3gTjMJYxYYOkH__22toa0bFOF4x ...\n",
            "Downloading test_kaggle_diabetic_retinopathy.zip.001 file, with file id: 1ixiYl3gTjMJYxYYOkH__22toa0bFOF4x ...\n",
            "Successfully downloaded the file: test_kaggle_diabetic_retinopathy.zip.001 to: ./test_kaggle_diabetic_retinopathy.zip.001 ...\n",
            "Found most recent version of: test_kaggle_diabetic_retinopathy.zip.002 file with file id: 110H1wy_8FqyyRGZOi-_a8PuUqgCKlTc1 ...\n",
            "Downloading test_kaggle_diabetic_retinopathy.zip.002 file, with file id: 110H1wy_8FqyyRGZOi-_a8PuUqgCKlTc1 ...\n",
            "Successfully downloaded the file: test_kaggle_diabetic_retinopathy.zip.002 to: ./test_kaggle_diabetic_retinopathy.zip.002 ...\n",
            "Found most recent version of: test_kaggle_diabetic_retinopathy.zip.003 file with file id: 1KiDf4ypex6A8VMkGWx-Br1jamOSFVB52 ...\n",
            "Downloading test_kaggle_diabetic_retinopathy.zip.003 file, with file id: 1KiDf4ypex6A8VMkGWx-Br1jamOSFVB52 ...\n",
            "Successfully downloaded the file: test_kaggle_diabetic_retinopathy.zip.003 to: ./test_kaggle_diabetic_retinopathy.zip.003 ...\n",
            "Found most recent version of: test_kaggle_diabetic_retinopathy.zip.004 file with file id: 1JJs7HYeQOhRJtrFSmCvtY9ZlkF3kf140 ...\n",
            "Downloading test_kaggle_diabetic_retinopathy.zip.004 file, with file id: 1JJs7HYeQOhRJtrFSmCvtY9ZlkF3kf140 ...\n",
            "Successfully downloaded the file: test_kaggle_diabetic_retinopathy.zip.004 to: ./test_kaggle_diabetic_retinopathy.zip.004 ...\n",
            "Found most recent version of: test_kaggle_diabetic_retinopathy.zip.005 file with file id: 1gsr19RsO-upACd2ItqwuRzfWCp_bCmph ...\n",
            "Downloading test_kaggle_diabetic_retinopathy.zip.005 file, with file id: 1gsr19RsO-upACd2ItqwuRzfWCp_bCmph ...\n",
            "Successfully downloaded the file: test_kaggle_diabetic_retinopathy.zip.005 to: ./test_kaggle_diabetic_retinopathy.zip.005 ...\n",
            "Found most recent version of: test_kaggle_diabetic_retinopathy.zip.006 file with file id: 10e8-o0I1mhzhF7CRmtq_gqL0GI-whUkd ...\n",
            "Downloading test_kaggle_diabetic_retinopathy.zip.006 file, with file id: 10e8-o0I1mhzhF7CRmtq_gqL0GI-whUkd ...\n",
            "Successfully downloaded the file: test_kaggle_diabetic_retinopathy.zip.006 to: ./test_kaggle_diabetic_retinopathy.zip.006 ...\n",
            "Found most recent version of: test_kaggle_diabetic_retinopathy.zip.007 file with file id: 11smEMQnb_pFRFKTm79ApuSmIakdBlSzK ...\n",
            "Downloading test_kaggle_diabetic_retinopathy.zip.007 file, with file id: 11smEMQnb_pFRFKTm79ApuSmIakdBlSzK ...\n",
            "Successfully downloaded the file: test_kaggle_diabetic_retinopathy.zip.007 to: ./test_kaggle_diabetic_retinopathy.zip.007 ...\n",
            "Found most recent version of: train_kaggle_diabetic_retinopathy.zip.001 file with file id: 1BVQpJS2bxeS2tUqLzHr4fG0PJUfp50Et ...\n",
            "Downloading train_kaggle_diabetic_retinopathy.zip.001 file, with file id: 1BVQpJS2bxeS2tUqLzHr4fG0PJUfp50Et ...\n",
            "Successfully downloaded the file: train_kaggle_diabetic_retinopathy.zip.001 to: ./train_kaggle_diabetic_retinopathy.zip.001 ...\n",
            "Found most recent version of: train_kaggle_diabetic_retinopathy.zip.002 file with file id: 1Awr_3aSUik7q3-fz5C_q9cKB515tXvt3 ...\n",
            "Downloading train_kaggle_diabetic_retinopathy.zip.002 file, with file id: 1Awr_3aSUik7q3-fz5C_q9cKB515tXvt3 ...\n",
            "Successfully downloaded the file: train_kaggle_diabetic_retinopathy.zip.002 to: ./train_kaggle_diabetic_retinopathy.zip.002 ...\n",
            "Found most recent version of: train_kaggle_diabetic_retinopathy.zip.003 file with file id: 107yseFE5FwbEJKvJotmNOnDzcRVqjfcR ...\n",
            "Downloading train_kaggle_diabetic_retinopathy.zip.003 file, with file id: 107yseFE5FwbEJKvJotmNOnDzcRVqjfcR ...\n",
            "Successfully downloaded the file: train_kaggle_diabetic_retinopathy.zip.003 to: ./train_kaggle_diabetic_retinopathy.zip.003 ...\n",
            "Found most recent version of: train_kaggle_diabetic_retinopathy.zip.004 file with file id: 17H_HCF_Bfr7c7pYsfS452_WpBftBQbpW ...\n",
            "Downloading train_kaggle_diabetic_retinopathy.zip.004 file, with file id: 17H_HCF_Bfr7c7pYsfS452_WpBftBQbpW ...\n",
            "Successfully downloaded the file: train_kaggle_diabetic_retinopathy.zip.004 to: ./train_kaggle_diabetic_retinopathy.zip.004 ...\n",
            "Found most recent version of: train_kaggle_diabetic_retinopathy.zip.005 file with file id: 1k1ZJF_7Qk-xS83WtNhQ904GcaHldyB_- ...\n",
            "Downloading train_kaggle_diabetic_retinopathy.zip.005 file, with file id: 1k1ZJF_7Qk-xS83WtNhQ904GcaHldyB_- ...\n",
            "Successfully downloaded the file: train_kaggle_diabetic_retinopathy.zip.005 to: ./train_kaggle_diabetic_retinopathy.zip.005 ...\n",
            "Found most recent version of: trainLabels_kaggle_diabetic_retinopathy.csv.zip file with file id: 1cDEBT-vip_z_hj6wl2jhVDcZDJZw4awE ...\n",
            "Downloading trainLabels_kaggle_diabetic_retinopathy.csv.zip file, with file id: 1cDEBT-vip_z_hj6wl2jhVDcZDJZw4awE ...\n",
            "Successfully downloaded the file: trainLabels_kaggle_diabetic_retinopathy.csv.zip to: ./trainLabels_kaggle_diabetic_retinopathy.csv.zip ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Rvv2jZkWxduK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "command = ['mkdir ./{}/'.format(dataset_id),\n",
        "           'mkdir ./{}/train/'.format(dataset_id),\n",
        "           'sudo apt-get install p7zip-full',\n",
        "           '7z e ./train_{}.zip.001 -o./{}/train/ -r'.format(dataset_id,\n",
        "                                                             dataset_id),\n",
        "           '7z e ./trainLabels_{}.csv.zip -o./{}/'.format(dataset_id,\n",
        "                                                        dataset_id),\n",
        "           'rm ./train_{}.zip.001'.format(dataset_id),\n",
        "           'mkdir ./{}/train/0/'.format(dataset_id),\n",
        "           'mkdir ./{}/train/1/'.format(dataset_id),\n",
        "           'mkdir ./{}/train/2/'.format(dataset_id),\n",
        "           'mkdir ./{}/train/3/'.format(dataset_id),\n",
        "           'mkdir ./{}/train/4/'.format(dataset_id)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9KMpJYfoxYUe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "20337c22-19e7-4f8a-ebb1-0f7a6a32e99f"
      },
      "cell_type": "code",
      "source": [
        "if setup:\n",
        "  execute_in_shell(command = command, \n",
        "                   verbose = True)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success running shell command: mkdir ./kaggle_diabetic_retinopathy/\n",
            "Success running shell command: mkdir ./kaggle_diabetic_retinopathy/train/\n",
            "Success running shell command: sudo apt-get install p7zip-full\n",
            "Success running shell command: 7z e ./train_kaggle_diabetic_retinopathy.zip.001 -o./kaggle_diabetic_retinopathy/train/ -r\n",
            "Success running shell command: 7z e ./trainLabels_kaggle_diabetic_retinopathy.csv.zip -o./kaggle_diabetic_retinopathy/\n",
            "Success running shell command: rm ./train_kaggle_diabetic_retinopathy.zip.001\n",
            "Success running shell command: mkdir ./kaggle_diabetic_retinopathy/train/0/\n",
            "Success running shell command: mkdir ./kaggle_diabetic_retinopathy/train/1/\n",
            "Success running shell command: mkdir ./kaggle_diabetic_retinopathy/train/2/\n",
            "Success running shell command: mkdir ./kaggle_diabetic_retinopathy/train/3/\n",
            "Success running shell command: mkdir ./kaggle_diabetic_retinopathy/train/4/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gfMolHdjGdJw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48f95d3c-0bcc-40c3-dc84-c697b3656463"
      },
      "cell_type": "code",
      "source": [
        "! ls ./kaggle_diabetic_retinopathy/validation/4/  | wc -l"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WojMsUa00YXq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Label the data"
      ]
    },
    {
      "metadata": {
        "id": "iEHe_b2hGb8M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "CSV_FILE = os.path.join('./{}/trainLabels.csv'.format(dataset_id))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nDNYT6sfGBel",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labels = pd.read_csv(CSV_FILE)\n",
        "file_names = labels.image\n",
        "file_labels = labels.level"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fr0fnFHOG1EY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(len(file_names)):\n",
        "  file_path = './{}/train/{}.jpeg'.format(dataset_id,\n",
        "                                     file_names[i])\n",
        "  labelled_dir = './{}/train/{}/'.format(dataset_id,\n",
        "                                         file_labels[i])\n",
        "  labelled_path = '{}/{}.jpeg'.format(labelled_dir,\n",
        "                                 file_names[i])\n",
        "  if os.path.exists(file_path) and os.path.exists(labelled_dir):\n",
        "    if verbose:\n",
        "      print (\"File found: {}\".format(file_path))\n",
        "    os.rename(file_path, labelled_path)\n",
        "    if verbose:\n",
        "      print (\"File moved from: {} ;to: {} ...\".format(file_path,\n",
        "                                                      labelled_path))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HzdiV493AVHD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "command = ['mkdir ./{}/validation/'.format(dataset_id),\n",
        "           'mkdir ./{}/validation/0/'.format(dataset_id),\n",
        "           'mkdir ./{}/validation/1/'.format(dataset_id),\n",
        "           'mkdir ./{}/validation/2/'.format(dataset_id),\n",
        "           'mkdir ./{}/validation/3/'.format(dataset_id),\n",
        "           'mkdir ./{}/validation/4/'.format(dataset_id),\n",
        "           'cd ./{}/train/0/ ; shuf -n 100 -e * | xargs -i mv {} ../../../{}/validation/0/'.format(dataset_id,\n",
        "                                                                                                          '{}',\n",
        "                                                                                                          dataset_id),\n",
        "\n",
        "           'cd ./{}/train/1/ ; shuf -n 100 -e * | xargs -i mv {} ../../../{}/validation/1/'.format(dataset_id,\n",
        "                                                                                                          '{}',\n",
        "                                                                                                          dataset_id),\n",
        "           'cd ./{}/train/2/ ; shuf -n 100 -e * | xargs -i mv {} ../../../{}/validation/2/'.format(dataset_id,\n",
        "                                                                                                    '{}',\n",
        "                                                                                                    dataset_id),\n",
        "           'cd ./{}/train/3/ ; shuf -n 100 -e * | xargs -i mv {} ../../../{}/validation/3/'.format(dataset_id,\n",
        "                                                                                                    '{}',\n",
        "                                                                                                    dataset_id),\n",
        "           'cd ./{}/train/4/ ; shuf -n 100 -e * | xargs -i mv {} ../../../{}/validation/4/'.format(dataset_id,\n",
        "                                                                                                    '{}',\n",
        "                                                                                                    dataset_id)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oGrykF4ZHs1o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "ada1b5e9-c848-4005-82e4-a4d1239a7afd"
      },
      "cell_type": "code",
      "source": [
        "if setup:\n",
        "  execute_in_shell(command = command, verbose = True)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success running shell command: mkdir ./kaggle_diabetic_retinopathy/validation/\n",
            "Success running shell command: mkdir ./kaggle_diabetic_retinopathy/validation/0/\n",
            "Success running shell command: mkdir ./kaggle_diabetic_retinopathy/validation/1/\n",
            "Success running shell command: mkdir ./kaggle_diabetic_retinopathy/validation/2/\n",
            "Success running shell command: mkdir ./kaggle_diabetic_retinopathy/validation/3/\n",
            "Success running shell command: mkdir ./kaggle_diabetic_retinopathy/validation/4/\n",
            "Success running shell command: cd ./kaggle_diabetic_retinopathy/train/0/ ; shuf -n 100 -e * | xargs -i mv {} ../../../kaggle_diabetic_retinopathy/validation/0/\n",
            "Success running shell command: cd ./kaggle_diabetic_retinopathy/train/1/ ; shuf -n 100 -e * | xargs -i mv {} ../../../kaggle_diabetic_retinopathy/validation/1/\n",
            "Success running shell command: cd ./kaggle_diabetic_retinopathy/train/2/ ; shuf -n 100 -e * | xargs -i mv {} ../../../kaggle_diabetic_retinopathy/validation/2/\n",
            "Success running shell command: cd ./kaggle_diabetic_retinopathy/train/3/ ; shuf -n 100 -e * | xargs -i mv {} ../../../kaggle_diabetic_retinopathy/validation/3/\n",
            "Success running shell command: cd ./kaggle_diabetic_retinopathy/train/4/ ; shuf -n 100 -e * | xargs -i mv {} ../../../kaggle_diabetic_retinopathy/validation/4/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7vlK1WSf6otu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_timestamp():\n",
        "    \"\"\" \n",
        "        A function to generate time-stamp information.\n",
        "        Calling the function returns a string formatted current system time.\n",
        "        Eg: 2018_10_10_10_10_10\n",
        "    \n",
        "        Example usage: generate_timestamp() \n",
        "    \"\"\"    \n",
        "    timestring = time.strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
        "    print (\"Time stamp generated: \" + timestring)\n",
        "    return timestring"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pZCZqWg4-62a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e471e270-d52b-46f7-9d1c-a954a8f793c9"
      },
      "cell_type": "code",
      "source": [
        "timestr = generate_timestamp()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time stamp generated: 2019_03_12-23_38_11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0XcwLfuV-7qv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def is_valid_file(parser, arg):\n",
        "    \"\"\"\n",
        "        A function that checks if a give file path contains a valid file or not.\n",
        "        \n",
        "        The function returns the full file path if there is a valid file persent.\n",
        "        If there is no valid file present at a file path location, it returns a parser error message.\n",
        "        \n",
        "        Takes two positional arguments: parser and arg\n",
        "        \n",
        "        Example usage: \n",
        "            import argsparse\n",
        "            \n",
        "            a = argparse.ArgumentParser()\n",
        "            a.add_argument(\"--file_path\", \n",
        "                              help = \"Check if a file exists in the specified file path ...\", \n",
        "                              dest = \"file_path\", \n",
        "                              required=False,\n",
        "                              type=lambda x: is_valid_file(a, x),\n",
        "                              nargs=1)\n",
        "            \n",
        "            args = a.parse_args()\n",
        "            \n",
        "            args = get_user_options()\n",
        "    \"\"\"\n",
        "    if not os.path.isfile(arg):\n",
        "        try:\n",
        "            parser.error(\"The file %s does not exist ...\" % arg)\n",
        "            return None\n",
        "        except:\n",
        "            if parser != None:\n",
        "                print (\"No valid argument parser found ...\")\n",
        "                print (\"The file %s does not exist ...\" % arg)\n",
        "                return None\n",
        "            else:\n",
        "                print (\"The file %s does not exist ...\" % arg)\n",
        "                return None\n",
        "    else:\n",
        "        return arg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xjLIEyJS_EU8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def is_valid_dir(parser, arg):\n",
        "    \"\"\"\n",
        "        This function checks if a directory exists or not.\n",
        "        It can be used inside the argument parser.\n",
        "        \n",
        "        Example usage: \n",
        "            \n",
        "            import argsparse\n",
        "            \n",
        "            a = argparse.ArgumentParser()\n",
        "            a.add_argument(\"--dir_path\", \n",
        "                              help = \"Check if a file exists in the specified file path ...\", \n",
        "                              dest = \"file_path\", \n",
        "                              required=False,\n",
        "                              type=lambda x: is_valid_dir(a, x),\n",
        "                              nargs=1)\n",
        "            \n",
        "            args = a.parse_args()\n",
        "            \n",
        "            args = get_user_options() \n",
        "    \"\"\"\n",
        "    if not os.path.isdir(arg):\n",
        "        try:\n",
        "            return parser.error(\"The folder %s does not exist ...\" % arg)\n",
        "        except:\n",
        "            if parser != None:\n",
        "                print (\"No valid argument parser found\")\n",
        "                print (\"The folder %s does not exist ...\" % arg)\n",
        "                return None\n",
        "            else:\n",
        "                print (\"The folder %s does not exist ...\" % arg)\n",
        "                return None\n",
        "    else:\n",
        "        return arg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-_atRJAa_HDB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def string_to_bool(val):\n",
        "    \"\"\"\n",
        "        A function that checks if an user argument is boolean or not.\n",
        "        \n",
        "        Example usage:\n",
        "            \n",
        "            \n",
        "                import argsparse\n",
        "            \n",
        "                a = argparse.ArgumentParser()\n",
        "                \n",
        "                a.add_argument(\"--some_bool_arg\", \n",
        "                   help = \"Specify a boolean argument ...\", \n",
        "                   dest = \"some_bool_arg\", \n",
        "                   required=False, \n",
        "                   default=[True], \n",
        "                   nargs=1, \n",
        "                   type = string_to_bool)\n",
        "                \n",
        "            args = a.parse_args()\n",
        "            \n",
        "            args = get_user_options()\n",
        "            \n",
        "    \"\"\"\n",
        "    if val.lower() in ('yes', 'true', 't', 'y', '1', 'yeah', 'yup'):\n",
        "        return True\n",
        "    elif val.lower() in ('no', 'false', 'f', 'n', '0', 'none', 'nope'):\n",
        "        return False\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError('Boolean value expected ...')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eIER8uML_KaC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def activation_val(val):\n",
        "    activation_function_options = ('hard_sigmoid',\n",
        "                                   'elu',\n",
        "                                   'linear',\n",
        "                                   'relu', \n",
        "                                   'selu', \n",
        "                                   'sigmoid',\n",
        "                                   'softmax',\n",
        "                                   'softplus',\n",
        "                                   'sofsign',\n",
        "                                   'tanh')\n",
        "    if val.lower() in activation_function_options:\n",
        "        return val\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError('Unexpected activation function. \\\n",
        "                                         \\nExpected values are:  {} ...'.format(activation_function_options))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9_i-ddbv_Ndd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loss_val(val):\n",
        "    loss_function_options = ('mean_squared_error',\n",
        "                             'mean_absolute_error',\n",
        "                             'mean_absolute_percentage_error',\n",
        "                             'mean_squared_logarithmic_error', \n",
        "                             'squared_hinge', \n",
        "                             'hinge',\n",
        "                             'categorical_hinge',\n",
        "                             'logcosh',\n",
        "                             'categorical_crossentropy',\n",
        "                             'sparse_categorical_crossentropy',\n",
        "                             'binary_crossentropy',\n",
        "                             'kullback_leibler_divergence',\n",
        "                             'poisson',\n",
        "                             'cosine_proximity')\n",
        "    if val.lower() in loss_function_options:\n",
        "        return val\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError('Unexpected loss function. \\\n",
        "                                         \\nExpected values are:  {} ...'.format(loss_function_options))\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hav6kyJZ_Ppw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_nb_files(directory):\n",
        "  if not os.path.exists(directory):\n",
        "    return 0\n",
        "  cnt = 0\n",
        "  for r, dirs, files in os.walk(directory):\n",
        "    for dr in dirs:\n",
        "      cnt += len(glob.glob(os.path.join(r, dr + \"/*\")))\n",
        "  return cnt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6MKLwcDo_X7G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def add_top_layer(args, enable_dropout, base_model, nb_classes):\n",
        "  \"\"\"\n",
        "    This functions adds a fully connected convolutional neural network layer to a base model.\n",
        "    \n",
        "    The required input arguments for this function are: args, base_model and nb_classes.\n",
        "        args: argument inputs the user arguments to be passed to the function,\n",
        "        base_model: argument inputs the base model architecture to be added to the top layer,\n",
        "        nb_classes: argument inputs the total number of classes for the output layer.    \n",
        "  \"\"\"\n",
        "  try:\n",
        "      dropout = float(args.dropout[0])\n",
        "      weight_decay = float(args.decay[0])\n",
        "  except:\n",
        "      dropout = DEFAULT_DROPOUT\n",
        "      print ('Invalid input for dropout ...')\n",
        "      \n",
        "  try:\n",
        "      activation = str(args.activation[0]).lower()\n",
        "      print ('Building model using activation function: ' + str(activation))\n",
        "  except:\n",
        "      activation = 'relu'\n",
        "      print ('Invalid input for activation function ...')\n",
        "      print ('Choice of activation functions: hard_sigmoid, elu, linear, relu, selu, sigmoid, softmax, softplus, sofsign, tanh ...')\n",
        "      print ('Building model using default activation function: relu')\n",
        "      \n",
        "  bm = base_model.output\n",
        "  \n",
        "  x = Dropout(dropout,\n",
        "              name='dropout_fc1')(bm,\n",
        "                       training=enable_dropout)\n",
        "  x = GlobalAveragePooling2D(name='gloablAveragePooling2D_fc1')(x)\n",
        "  x = Dropout(dropout,\n",
        "              name='dropout_fc2')(x,\n",
        "                       training=enable_dropout)\n",
        "  x = BatchNormalization(name='batchNormalization_fc1')(x)\n",
        "  x = Dense(FC_SIZE, \n",
        "            activation=activation,\n",
        "            kernel_regularizer=l2(weight_decay),\n",
        "            name='dense_fc1')(x)\n",
        "  x = Dropout(dropout,\n",
        "              name='dropout_fc3')(x,\n",
        "                       training=enable_dropout)\n",
        "  \n",
        "  x1 = Dense(FC_SIZE, \n",
        "             activation=activation,\n",
        "             kernel_regularizer=l2(weight_decay),\n",
        "             name=\"dense_fc2\")(x)\n",
        "  x1 = Dropout(dropout,\n",
        "               name = 'dropout_fc4')(x1, \n",
        "                                  training=enable_dropout)\n",
        "  x1 = BatchNormalization(name=\"batchNormalization_fc2\")(x1)\n",
        "  x1 = Dense(FC_SIZE, \n",
        "             activation=activation, \n",
        "             kernel_regularizer=l2(weight_decay),\n",
        "             name=\"dense_fc3\")(x1)\n",
        "  x1 = Dropout(dropout,\n",
        "               name = 'dropout_fc5')(x1, \n",
        "                                  training=enable_dropout)\n",
        "\n",
        "  x2 = Dense(FC_SIZE, \n",
        "             activation=activation, \n",
        "             kernel_regularizer=l2(weight_decay),\n",
        "             name=\"dense_fc4\")(x)\n",
        "  x2 = Dropout(dropout,\n",
        "               name = 'dropout_fc6')(x2, \n",
        "                                  training=enable_dropout)\n",
        "  x2 = BatchNormalization(name=\"batchNormalization_fc3\")(x2)\n",
        "  x2 = Dense(FC_SIZE, \n",
        "             activation=activation, \n",
        "             kernel_regularizer=l2(weight_decay),\n",
        "             name=\"dense_fc5\")(x2)\n",
        "  x2 = Dropout(dropout,\n",
        "               name = 'dropout_fc7')(x2, \n",
        "                                  training=enable_dropout)\n",
        "\n",
        "  x12 = concatenate([x1, x2], name = 'mixed11')\n",
        "  x12 = Dropout(dropout,\n",
        "                name = 'dropout_fc8')(x12, \n",
        "                                   training=enable_dropout)\n",
        "  x12 = Dense(FC_SIZE//16, \n",
        "              activation=activation, \n",
        "              kernel_regularizer=l2(weight_decay),\n",
        "              name = 'dense_fc6')(x12)\n",
        "  x12 = Dropout(dropout,\n",
        "                name = 'dropout_fc9')(x12, \n",
        "                                   training=enable_dropout)\n",
        "  x12 = BatchNormalization(name=\"batchNormalization_fc4\")(x12)\n",
        "  x12 = Dense(FC_SIZE//32, \n",
        "              activation=activation, \n",
        "              kernel_regularizer=l2(weight_decay),\n",
        "              name = 'dense_fc7')(x12)\n",
        "  x12 = Dropout(dropout,\n",
        "                name = 'dropout_fc10')(x12, \n",
        "                                   training=enable_dropout)\n",
        "  \n",
        "  x3 = Dropout(dropout,\n",
        "              name='dropout_fc11')(bm,\n",
        "                       training=enable_dropout)\n",
        "  x3 = GlobalAveragePooling2D( name = 'globalAveragePooling2D_fc2')(x3)\n",
        "  x3 = Dense(FC_SIZE//2, \n",
        "             activation=activation, \n",
        "             kernel_regularizer=l2(weight_decay),\n",
        "             name = 'dense_fc8')(x3)\n",
        "  x3 = Dropout(dropout,\n",
        "               name = 'dropout_fc12')(x3, \n",
        "                                  training=enable_dropout)\n",
        "  x3 = BatchNormalization(name=\"batchNormalization_fc5\")(x3)\n",
        "  x3 = Dense(FC_SIZE//2, \n",
        "             activation=activation, \n",
        "             kernel_regularizer=l2(weight_decay),\n",
        "             name = 'dense_fc9')(x3)\n",
        "  x3 = Dropout(dropout,\n",
        "               name = 'dropout_fc13')(x3, \n",
        "                                  training=enable_dropout)\n",
        "  \n",
        "  xout = concatenate([x12, x3], name ='mixed12')\n",
        "  xout = Dense(FC_SIZE//32, \n",
        "               activation= activation, \n",
        "               kernel_regularizer=l2(weight_decay),\n",
        "               name = 'dense_fc10')(xout)\n",
        "  xout = Dropout(dropout,\n",
        "                 name = 'dropout_fc14')(xout, \n",
        "                                     training=enable_dropout)\n",
        "  \n",
        "  predictions = Dense(nb_classes,           \\\n",
        "                      activation='sigmoid', \\\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      name='prediction')(xout) # Softmax output layer\n",
        "  \n",
        "  model = Model(inputs=base_model.input, \n",
        "                outputs=predictions)\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l77GW_9O_ajP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def finetune_model(model, optimizer, loss, NB_FROZEN_LAYERS):\n",
        "  \"\"\"\n",
        "      A function that freezes the bottom NB_LAYERS and retrain the remaining top layers.\n",
        "      \n",
        "      The required input arguments for this function are: model, optimizer and NB_FROZEN_LAYERS.\n",
        "          model: inputs a model architecture with base layers to be frozen during training,\n",
        "          optimizer: inputs a choice of optimizer value for compiling the model,\n",
        "          loss: inputs a choice for loss function used for compiling the model,\n",
        "          NB_FROZEN_LAYERS: inputs a number that selects the total number of base layers to be frozen during training.\n",
        "      \n",
        "  \"\"\"\n",
        "                     \n",
        "  for layer in model.layers[:NB_FROZEN_LAYERS]:\n",
        "     layer.trainable = False\n",
        "  for layer in model.layers[NB_FROZEN_LAYERS:]:\n",
        "     layer.trainable = True\n",
        "  model.compile(optimizer=optimizer, \n",
        "                loss=loss, \n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0eM5ok7X_d_1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def transferlearn_model(model, base_model, optimizer, loss):\n",
        "  \"\"\"\n",
        "     Function that freezes the base layers to train just the top layer.\n",
        "     \n",
        "     This function takes three positional arguments:\n",
        "         model: specifies the input model,\n",
        "         base_model: specifies the base model architecture,\n",
        "         optimizer: optimizer function for training the model,\n",
        "         loss: loss function for compiling the model\n",
        "     \n",
        "     Example usage:\n",
        "         transferlearn_model(model, base_model, optimizer)\n",
        "  \"\"\"\n",
        "  for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "  model.compile(optimizer=optimizer, \n",
        "                loss=loss, \n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RXCHAeGz_gKi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_model(args, name, model):\n",
        "    file_loc = args.output_dir[0]\n",
        "    file_pointer = os.path.join(file_loc+\"//trained_\"+ timestr)\n",
        "    model.save_weights(os.path.join(file_pointer + \"_weights\"+str(name)+\".model\"))\n",
        "    \n",
        "    model_json = model.to_json()                                                # Serialize model to JSON\n",
        "    with open(os.path.join(file_pointer+\"_config\"+str(name)+\".json\"), \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "    print (\"Saved the trained model weights to: \" + \n",
        "           str(os.path.join(file_pointer + \"_weights\"+str(name)+\".model\")))\n",
        "    print (\"Saved the trained model configuration as a json file to: \" + \n",
        "           str(os.path.join(file_pointer+\"_config\"+str(name)+\".json\")))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "snR81aYX_ird",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_labels(args):\n",
        "    file_loc = args.output_dir[0]\n",
        "    file_pointer = os.path.join(file_loc+\"//trained_labels\")\n",
        "    \n",
        "    data_dir = args.train_dir[0]\n",
        "    val_dir_ = args.val_dir[0]\n",
        "    \n",
        "    dt = defaultdict(list)\n",
        "    dv = defaultdict(list)\n",
        "    \n",
        "    for root, subdirs, files in os.walk(data_dir):\n",
        "        for filename in files:\n",
        "            file_path = os.path.join(root, filename)\n",
        "            assert file_path.startswith(data_dir)\n",
        "            suffix = file_path[len(data_dir):]\n",
        "            suffix = suffix.lstrip(\"/\")\n",
        "            label = suffix.split(\"/\")[0]\n",
        "            dt[label].append(file_path)\n",
        "            \n",
        "    for root, subdirs, files in os.walk(val_dir_):\n",
        "        for filename in files:\n",
        "            file_path = os.path.join(root, filename)\n",
        "            assert file_path.startswith(val_dir_)\n",
        "            suffix = file_path[len(val_dir_):]\n",
        "            suffix = suffix.lstrip(\"/\")\n",
        "            label = suffix.split(\"/\")[0]\n",
        "            dv[label].append(file_path)\n",
        "\n",
        "    labels = sorted(dt.keys())\n",
        "    val_labels = sorted(dv.keys())\n",
        "    \n",
        "    if set(labels) == set (val_labels):\n",
        "        print(\"\\nTraining labels: \" + str(labels))\n",
        "        print(\"\\nValidation labels: \" + str(val_labels))\n",
        "        with open(os.path.join(file_pointer+\".json\"), \"w\") as json_file:\n",
        "            json.dump(labels, json_file)\n",
        "    else:\n",
        "      print(\"\\nTraining labels: \" + str(labels))\n",
        "      print(\"\\nValidation labels: \" + str(val_labels))\n",
        "      print (\"Mismatched training and validation data labels ...\")\n",
        "      print (\"Sub-folder names do not match between training and validation directories ...\")\n",
        "      sys.exit(1)\n",
        "\n",
        "    return labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bf_nUUK2_lIK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def normalize(args, \n",
        "              labels, \n",
        "              move = False, \n",
        "              sub_sample = False):\n",
        "    if args.normalize[0] and os.path.exists(args.root_dir[0]):      \n",
        "        commands = [\"rm -r {}/.tmp_train/\".format(args.root_dir[0]),\n",
        "                    \"rm -r {}/.tmp_validation/\".format(args.root_dir[0]),\n",
        "                    \"mkdir {}/.tmp_train/\".format(args.root_dir[0]),\n",
        "                    \"mkdir {}/.tmp_validation/\".format(args.root_dir[0])]\n",
        "        execute_in_shell(command=commands,\n",
        "                         verbose=verbose)\n",
        "        del commands\n",
        "        \n",
        "        mk_train_folder = \"mkdir -p {}/.tmp_train/\".format(args.root_dir[0]) + \"{}\"\n",
        "        mk_val_folder = \"mkdir -p {}/.tmp_validation/\".format(args.root_dir[0]) + \"{}\"\n",
        "        \n",
        "        train_class_sizes = []\n",
        "        val_class_sizes = []\n",
        "        \n",
        "        for label in labels:\n",
        "            train_class_sizes.append(len(glob.glob(args.train_dir[0] + \"/{}/*\".format(label))))\n",
        "            val_class_sizes.append(len(glob.glob(args.val_dir[0] + \"/{}/*\".format(label))))\n",
        "        \n",
        "        train_size = min(train_class_sizes)\n",
        "        val_size = min(val_class_sizes)\n",
        "        \n",
        "        if sub_sample and 0 <= args.train_sub_sample[0] <=1 and 0 <= args.val_sub_sample[0] <=1 :\n",
        "            train_size = int(train_size * args.train_sub_sample[0])\n",
        "            val_size = int(val_size * args.val_sub_sample[0])\n",
        "        \n",
        "        print (\"Normalized training class size {}\".format(train_size))\n",
        "        print (\"Normalized validation class size {}\".format(val_size))\n",
        "        \n",
        "        for label in labels:\n",
        "            commands = [mk_train_folder.format(label),\n",
        "                        mk_val_folder.format(label)]\n",
        "        \n",
        "            execute_in_shell(command=commands,\n",
        "                             verbose=verbose)\n",
        "            del commands\n",
        "        \n",
        "        commands = []\n",
        "        \n",
        "        for label in labels:\n",
        "            train_images = (glob.glob('{}/{}/*.*'.format(args.train_dir[0], label), recursive=True))\n",
        "            val_images = (glob.glob('{}/{}/*.*'.format(args.val_dir[0], label), recursive=True))\n",
        "            \n",
        "            sys_rnd = random.SystemRandom()\n",
        "            \n",
        "            if move:\n",
        "              cmd = 'mv'\n",
        "            else:\n",
        "              cmd = 'cp'\n",
        "            \n",
        "            for file in sys_rnd.sample(train_images, train_size):\n",
        "                if os.path.exists(file):\n",
        "                    commands.append('{} {} ./.tmp_train/{}/'.format(cmd, file, label))\n",
        "            \n",
        "            for file in sys_rnd.sample(val_images, val_size):\n",
        "                if os.path.exists(file):\n",
        "                    commands.append('{} {} ./.tmp_validation/{}/'.format(cmd, file, label))\n",
        "                \n",
        "            p = Process(target=execute_in_shell, args=([commands]))\n",
        "            p.start()\n",
        "            p.join()\n",
        "        print (\"\\nData normalization pipeline completed successfully ...\")\n",
        "    else:\n",
        "        print (\"\\nFailed to initiate data normalization pipeline ...\")\n",
        "        return False    \n",
        "    return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dRVnEBhJ_oXx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_plot(args, name, model_train):\n",
        "    gen_plot = args.plot[0]\n",
        "    if gen_plot==True:\n",
        "        plot_training(args, name, model_train)\n",
        "    else:\n",
        "        print (\"\\nNo training summary plots generated ...\")\n",
        "        print (\"Set: --plot True for creating training summary plots\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0CGwyvxX_rTj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_training(args, name, history):\n",
        "  output_loc = args.output_dir[0]\n",
        "  \n",
        "  output_file_acc = os.path.join(output_loc+\n",
        "                                 \"//training_plot_acc_\" + \n",
        "                                 timestr+str(name)+\".png\")\n",
        "  output_file_loss = os.path.join(output_loc+\n",
        "                                  \"//training_plot_loss_\" + \n",
        "                                  timestr+str(name)+\".png\")\n",
        "  fig_acc = plt.figure()\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  fig_acc.savefig(output_file_acc, dpi=fig_acc.dpi)\n",
        "  print (\"Successfully created the training accuracy plot: \" \n",
        "         + str(output_file_acc))\n",
        "  plt.close()\n",
        "\n",
        "  fig_loss = plt.figure()\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  fig_loss.savefig(output_file_loss, dpi=fig_loss.dpi)\n",
        "  print (\"Successfully created the loss function plot: \" \n",
        "         + str(output_file_loss))\n",
        "  plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lKTLBtvK_unj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def select_optimizer(args):\n",
        "  optimizer_val = args.optimizer_val[0]\n",
        "  lr = args.learning_rate[0]\n",
        "  decay = args.decay[0]\n",
        "  epsilon = args.epsilon[0]\n",
        "  rho = args.rho[0]\n",
        "  beta_1 = args.beta_1[0]\n",
        "  beta_2 = args.beta_2[0]\n",
        "  \n",
        "  if optimizer_val.lower() == 'sgd' :\n",
        "    optimizer = SGD(lr=lr,       \\\n",
        "                    decay=decay, \\\n",
        "                    momentum=1,  \\\n",
        "                    nesterov=False)\n",
        "    print (\"Using SGD as the optimizer ...\")\n",
        "  elif optimizer_val.lower() == 'nsgd':\n",
        "    optimizer = SGD(lr=lr,      \\\n",
        "                    decay=decay,\\\n",
        "                    momentum=1, \\\n",
        "                    nesterov=True)\n",
        "    print (\"Using SGD as the optimizer with Nesterov momentum ...\")\n",
        "  elif optimizer_val.lower() == 'rms' \\\n",
        "       or \\\n",
        "       optimizer_val.lower() == 'rmsprop':\n",
        "    optimizer = RMSprop(lr=lr,          \\\n",
        "                        rho=rho,        \\\n",
        "                        epsilon=epsilon,\\\n",
        "                        decay=decay)\n",
        "    print (\"Using RMSProp as the optimizer ...\")\n",
        "  elif optimizer_val.lower() == 'ada' \\\n",
        "       or \\\n",
        "       optimizer_val.lower() == 'adagrad':\n",
        "    optimizer = Adagrad(lr=lr,           \\\n",
        "                        epsilon=epsilon, \\\n",
        "                        decay=decay)\n",
        "    print (\"Using Adagrad as the optimizer ...\")\n",
        "  elif optimizer_val.lower() == 'adelta' \\\n",
        "       or \\\n",
        "       optimizer_val.lower() == 'adadelta':\n",
        "    optimizer = Adadelta(lr=lr,           \\\n",
        "                         rho=rho,         \\\n",
        "                         epsilon=epsilon, \\\n",
        "                         decay=decay)\n",
        "    print (\"Using Adadelta as the optimizer ...\")\n",
        "  elif optimizer_val.lower() == 'adam':\n",
        "    optimizer = Adam(lr=lr,           \\\n",
        "                     beta_1=beta_1,   \\\n",
        "                     beta_2=beta_2,    \\\n",
        "                     epsilon=epsilon, \\\n",
        "                     decay=decay,     \\\n",
        "                     amsgrad=False)\n",
        "    print (\"Using Adam as the optimizer ...\")\n",
        "    print (\"Optimizer parameters (recommended default): \")\n",
        "    print (\"\\n lr={} (0.001),     \\\n",
        "            \\n beta_1={} (0.9),   \\\n",
        "            \\n beta_2={} (0.999), \\\n",
        "            \\n epsilon={} (1e-08), \\\n",
        "            \\n decay={} (0.0)\".format(lr, \n",
        "                                      beta_1, \n",
        "                                      beta_2, \n",
        "                                      epsilon, \n",
        "                                      decay))\n",
        "  elif optimizer_val.lower() == 'amsgrad':\n",
        "    optimizer = Adam(lr=lr,           \\\n",
        "                     beta_1=beta_1,   \\\n",
        "                     beta_2=beta_2,    \\\n",
        "                     epsilon=epsilon, \\\n",
        "                     decay=decay,     \\\n",
        "                     amsgrad=True)\n",
        "    print (\"Using AmsGrad variant of Adam as the optimizer ...\")\n",
        "    print (\"Optimizer parameters (recommended default): \")\n",
        "    print (\"\\n lr={} (0.001),     \\\n",
        "            \\n beta_1={} (0.9),   \\\n",
        "            \\n beta_2={} (0.999), \\\n",
        "            \\n epsilon={} (1e-08), \\\n",
        "            \\n decay={} (0.0)\".format(lr, \n",
        "                                      beta_1, \n",
        "                                      beta_2, \n",
        "                                      epsilon, \n",
        "                                      decay))\n",
        "  elif optimizer_val.lower() == 'adamax':  \n",
        "    optimizer = Adamax(lr=lr,           \\\n",
        "                       beta_1=beta_1,   \\\n",
        "                       beta_2=beta_2,    \\\n",
        "                       epsilon=epsilon, \\\n",
        "                       decay=decay)\n",
        "    print (\"Using Adamax variant of Adam as the optimizer ...\")\n",
        "    print (\"Optimizer parameters (recommended default): \")\n",
        "    print (\"\\n lr={} (0.002),     \\\n",
        "            \\n beta_1={} (0.9),   \\\n",
        "            \\n beta_2={} (0.999), \\\n",
        "            \\n epsilon={} (1e-08), \\\n",
        "            \\n schedule_decay={} (0.0)\".format(lr, \n",
        "                                               beta_1, \n",
        "                                               beta_2, \n",
        "                                               epsilon, \n",
        "                                               decay))\n",
        "  elif optimizer_val.lower() == 'nadam':  \n",
        "    optimizer = Nadam(lr=lr,            \\\n",
        "                      beta_1=beta_1,    \\\n",
        "                      beta_2=beta_2,     \\\n",
        "                      epsilon=epsilon,  \\\n",
        "                      schedule_decay=decay)\n",
        "    print (\"Using Nesterov Adam optimizer ...\\\n",
        "           \\n decay arguments is passed on to schedule_decay variable ...\")\n",
        "    print (\"Optimizer parameters (recommended default): \")\n",
        "    print (\"\\n lr={} (0.002),     \\\n",
        "            \\n beta_1={} (0.9),   \\\n",
        "            \\n beta_2={} (0.999), \\\n",
        "            \\n epsilon={} (1e-08), \\\n",
        "            \\n schedule_decay={} (0.004)\".format(lr, \n",
        "                                                 beta_1, \n",
        "                                                 beta_2, \n",
        "                                                 epsilon, \n",
        "                                                 decay))\n",
        "  else:\n",
        "      optimizer = DEFAULT_OPTIMIZER\n",
        "      print (\"Using stochastic gradient descent with Nesterov momentum ('nsgd') as the default optimizer ...\")\n",
        "      print (\"Options for optimizer are: 'sgd',        \\\n",
        "                                         \\n'nsgd',     \\\n",
        "                                         \\n'rmsprop',  \\\n",
        "                                         \\n'adagrad',  \\\n",
        "                                         \\n'adadelta', \\\n",
        "                                         \\n'adam',     \\\n",
        "                                         \\n'nadam',    \\\n",
        "                                         \\n'amsgrad',  \\\n",
        "                                         \\n'adamax' ...\")\n",
        "  return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L96oo8E1_yVw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def process_model(args, \n",
        "                  model, \n",
        "                  base_model, \n",
        "                  optimizer, \n",
        "                  loss, \n",
        "                  checkpointer_savepath):\n",
        "  load_weights_ = args.load_weights[0]\n",
        "  fine_tune_model = args.fine_tune[0]\n",
        "  load_checkpoint = args.load_checkpoint[0]\n",
        "   \n",
        "  if load_weights_ == True:     \n",
        "      try:\n",
        "          with open(args.config_file[0]) as json_file:\n",
        "              model_json = json_file.read()\n",
        "          model = model_from_json(model_json)\n",
        "      except:\n",
        "          model = model\n",
        "      try:\n",
        "          model.load_weights(args.weights_file[0])\n",
        "          print (\"\\nLoaded model weights from: \" + str(args.weights_file[0]))\n",
        "      except:\n",
        "          print (\"\\nError loading model weights ...\")\n",
        "          print (\"Tabula rasa ...\")\n",
        "          print (\"Loaded default model weights ...\")\n",
        "  elif load_checkpoint == True and os.path.exists(checkpointer_savepath):     \n",
        "      try:\n",
        "          model = load_model(checkpointer_savepath)\n",
        "          print (\"\\nLoaded model from checkpoint: \" + str(checkpointer_savepath))\n",
        "      except:\n",
        "          if os.path.exists(args.saved_chkpnt[0]):\n",
        "            model = load_model(args.saved_chkpnt[0])\n",
        "            print ('\\nLoaded saved checkpoint file ...')\n",
        "          else:\n",
        "            print (\"\\nError loading model checkpoint ...\")\n",
        "            print (\"Tabula rasa ...\")\n",
        "            print (\"Loaded default model weights ...\")\n",
        "  else:\n",
        "      model = model\n",
        "      print (\"\\nTabula rasa ...\")\n",
        "      print (\"Loaded default model weights ...\")\n",
        " \n",
        "  try:\n",
        "      NB_FROZEN_LAYERS = args.frozen_layers[0]\n",
        "  except:\n",
        "      NB_FROZEN_LAYERS = DEFAULT_NB_LAYERS_TO_FREEZE\n",
        "      \n",
        "  if fine_tune_model == True:\n",
        "      print (\"\\nFine tuning Inception architecture ...\")\n",
        "      print (\"Frozen layers: \" + str(NB_FROZEN_LAYERS))\n",
        "      model = finetune_model(model, optimizer, loss, NB_FROZEN_LAYERS)\n",
        "  else:\n",
        "      print (\"\\nTransfer learning using Inception architecture ...\")\n",
        "      model = transferlearn_model(model, base_model, optimizer, loss)\n",
        "      \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0ywN2xGz_2_a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def process_images(args):  \n",
        "  train_aug = args.train_aug[0] \n",
        "  test_aug = args.test_aug[0] \n",
        "   \n",
        "  if str((args.base_model[0]).lower()) == 'inceptionv4' or  \\\n",
        "     str((args.base_model[0]).lower()) == 'inception_v4' or \\\n",
        "     str((args.base_model[0]).lower()) == 'inception_resnet':\n",
        "      preprocess_input = preprocess_input_inceptionv4\n",
        "  else:\n",
        "      preprocess_input = preprocess_input_inceptionv3\n",
        "  \n",
        "  if train_aug==True:\n",
        "    try:\n",
        "        train_rotation_range = args.train_rot[0]\n",
        "        train_width_shift_range = args.train_w_shift[0]\n",
        "        train_height_shift_range = args.train_ht_shift[0]\n",
        "        train_shear_range = args.train_shear[0]\n",
        "        train_zoom_range = args.train_zoom[0]\n",
        "        train_vertical_flip = args.train_vflip[0]\n",
        "        train_horizontal_flip = args.train_hflip[0]\n",
        "    except:\n",
        "        train_rotation_range = 30\n",
        "        train_width_shift_range = 0.2\n",
        "        train_height_shift_range = 0.2\n",
        "        train_shear_range = 0.2\n",
        "        train_zoom_range = 0.2\n",
        "        train_vertical_flip = True\n",
        "        train_horizontal_flip = True\n",
        "        print (\"\\nFailed to load custom training image augmentation parameters ...\")\n",
        "        print (\"Loaded pre-set defaults ...\")\n",
        "        print (\"To switch off image augmentation during training, set --train_augmentation flag to False\")\n",
        "        \n",
        "    train_datagen =  ImageDataGenerator(preprocessing_function=preprocess_input,\n",
        "                                        rotation_range=train_rotation_range,\n",
        "                                        width_shift_range=train_width_shift_range,\n",
        "                                        height_shift_range=train_height_shift_range,\n",
        "                                        shear_range=train_shear_range,\n",
        "                                        zoom_range=train_zoom_range,\n",
        "                                        vertical_flip=train_vertical_flip,                                  \n",
        "                                        horizontal_flip=train_horizontal_flip)\n",
        "    print (\"\\nCreated image augmentation pipeline for training images ...\")     \n",
        "    print (\"Image augmentation parameters for training images: \\\n",
        "          \\n image rotation range = {},\\\n",
        "          \\n width shift range = {},\\\n",
        "          \\n height shift range = {}, \\\n",
        "          \\n shear range = {} ,\\\n",
        "          \\n zoom range = {}, \\\n",
        "          \\n enable vertical flip = {}, \\\n",
        "          \\n enable horizontal flip = {}\".format(train_rotation_range,\n",
        "                                                   train_width_shift_range,\n",
        "                                                   train_height_shift_range,\n",
        "                                                   train_shear_range,\n",
        "                                                   train_zoom_range,\n",
        "                                                   train_vertical_flip,\n",
        "                                                   train_horizontal_flip))\n",
        "  else:\n",
        "      train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "  \n",
        "  if test_aug==True:\n",
        "      try:\n",
        "        test_rotation_range = args.test_rot[0]\n",
        "        test_width_shift_range = args.test_w_shift[0]\n",
        "        test_height_shift_range = args.test_ht_shift[0]\n",
        "        test_shear_range = args.test_shear[0]\n",
        "        test_zoom_range = args.test_zoom[0]\n",
        "        test_vertical_flip = args.test_vflip[0]\n",
        "        test_horizontal_flip = args.test_hflip[0]\n",
        "      except:\n",
        "        test_rotation_range = 30\n",
        "        test_width_shift_range = 0.2\n",
        "        test_height_shift_range = 0.2\n",
        "        test_shear_range = 0.2\n",
        "        test_zoom_range = 0.2\n",
        "        test_vertical_flip = True\n",
        "        test_horizontal_flip = True\n",
        "        print (\"\\nFailed to load custom training image augmentation parameters ...\")\n",
        "        print (\"Loaded pre-set defaults ...\")\n",
        "        print (\"To switch off image augmentation during training, set --train_augmentation flag to False\")\n",
        "      test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
        "                                        rotation_range=test_rotation_range,\n",
        "                                        width_shift_range=test_width_shift_range,\n",
        "                                        height_shift_range=test_height_shift_range,\n",
        "                                        shear_range=test_shear_range,\n",
        "                                        zoom_range=test_zoom_range,\n",
        "                                        vertical_flip=test_vertical_flip,\n",
        "                                        horizontal_flip=test_horizontal_flip)\n",
        "      print (\"\\nCreated image augmentation pipeline for training images ...\")     \n",
        "      print (\"\\nImage augmentation parameters for training images:\")\n",
        "      print( \"\\n image rotation range = {},\\\n",
        "              \\n width shift range = {},\\\n",
        "              \\n height shift range = {}, \\\n",
        "              \\n shear range = {} ,\\\n",
        "              \\n zoom range = {}, \\\n",
        "              \\n enable vertical flip = {}, \\\n",
        "              \\n enable horizontal flip = {}\".format(test_rotation_range,\n",
        "                                                     test_width_shift_range,\n",
        "                                                     test_height_shift_range,\n",
        "                                                     test_shear_range,\n",
        "                                                     test_zoom_range,\n",
        "                                                     test_vertical_flip,\n",
        "                                                     test_horizontal_flip))\n",
        "  else:\n",
        "      test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "  return [train_datagen, test_datagen]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H8IK-4kp_5t1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gen_model(args, enable_dropout):\n",
        "  if str((args.base_model[0]).lower()) == 'inceptionv4' or  \\\n",
        "     str((args.base_model[0]).lower()) == 'inception_v4' or \\\n",
        "     str((args.base_model[0]).lower()) == 'inception_resnet':\n",
        "      base_model = InceptionResNetV2(weights='imagenet', \\\n",
        "                                     include_top=False)\n",
        "      base_model_name = 'Inception version 4'\n",
        "  else:\n",
        "      base_model = InceptionV3(weights='imagenet', \n",
        "                               include_top=False)\n",
        "      base_model_name = 'Inception version 3'\n",
        "  print ('\\nBase model: ' + str(base_model_name))\n",
        "  nb_classes = len(glob.glob(args.train_dir[0] + \"/*\"))\n",
        "  model = add_top_layer(args, \n",
        "                        enable_dropout,\n",
        "                        base_model, \n",
        "                        nb_classes)\n",
        "  print (\"New top layer added to: \" + str(base_model_name))\n",
        "  return [model, base_model]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gimObmca_9J1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(args): \n",
        "  \"\"\"\n",
        "    A function that takes the user arguments and initiates a training session of the neural network.\n",
        "    \n",
        "    This function takes only one input: args\n",
        "    \n",
        "    Example usage:\n",
        "            \n",
        "        if train_model == True:\n",
        "            print (\"Training sesssion initiated ...\")\n",
        "            train(args)\n",
        "  \"\"\"    \n",
        "  \n",
        "  if not os.path.exists(args.output_dir[0]):\n",
        "    os.makedirs(args.output_dir[0])\n",
        "    \n",
        "  optimizer  = select_optimizer(args)\n",
        "  loss = args.loss[0]\n",
        "  checkpointer_savepath = os.path.join(args.output_dir[0]     +       \n",
        "                                       '/checkpoint/Transfer_learn_' +       \n",
        "                                       str(IM_WIDTH)  + '_'  + \n",
        "                                       str(IM_HEIGHT) + '_'  + '.h5')\n",
        "  \n",
        "  nb_train_samples = get_nb_files(args.train_dir[0])\n",
        "  nb_classes = len(glob.glob(args.train_dir[0] + \"/*\"))\n",
        "  \n",
        "  print (\"\\nTotal number of training samples = \" + str(nb_train_samples))\n",
        "  print (\"Number of training classes = \" + str(nb_classes))\n",
        "  \n",
        "  nb_val_samples = get_nb_files(args.val_dir[0])\n",
        "  nb_val_classes = len(glob.glob(args.val_dir[0] + \"/*\"))\n",
        "  \n",
        "  print (\"\\nTotal number of validation samples = \" + str(nb_val_samples))\n",
        "  print (\"Number of validation classes = \" + str(nb_val_classes))\n",
        "  \n",
        "  if nb_val_classes == nb_classes:\n",
        "      print (\"\\nInitiating training session ...\")\n",
        "  else:\n",
        "      print (\"\\nMismatched number of training and validation data classes ...\")\n",
        "      print (\"Unequal number of sub-folders found between train and validation directories ...\")\n",
        "      print (\"Each sub-folder in train and validation directroies are treated as a separate class ...\")\n",
        "      print (\"Correct this mismatch and re-run ...\")\n",
        "      print (\"\\nNow exiting ...\")\n",
        "      sys.exit(1)\n",
        "      \n",
        "  nb_epoch = int(args.epoch[0])\n",
        "  batch_size = int(args.batch[0])    \n",
        "  \n",
        "  [train_datagen, validation_datagen] = process_images(args)\n",
        "  \n",
        "  labels = generate_labels(args)\n",
        "  \n",
        "  train_dir = args.train_dir[0]\n",
        "  val_dir = args.val_dir[0]\n",
        "  \n",
        "  if args.normalize[0] and os.path.exists(args.root_dir[0]):\n",
        "      normalize(args, \n",
        "                labels, \n",
        "                move = False,\n",
        "                sub_sample = args.sub_sample[0])\n",
        "      train_dir = os.path.join(args.root_dir[0] + \n",
        "                               str ('/.tmp_train/'))\n",
        "      val_dir = os.path.join(args.root_dir[0] + \n",
        "                             str ('/.tmp_validation/'))\n",
        "      \n",
        "  print (\"\\nGenerating training data: ... \")\n",
        "  train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                      target_size=(IM_WIDTH, IM_HEIGHT),\n",
        "                                                      batch_size=batch_size,\n",
        "                                                      class_mode='categorical')\n",
        "  \n",
        "  print (\"\\nGenerating validation data: ... \")\n",
        "  validation_generator = validation_datagen.flow_from_directory(val_dir,\n",
        "                                                          target_size=(IM_WIDTH, IM_HEIGHT),\n",
        "                                                          batch_size=batch_size,\n",
        "                                                          class_mode='categorical')\n",
        "  \n",
        "  \n",
        "  [model, base_model] = gen_model(args, enable_dropout)\n",
        "    \n",
        "  model = process_model(args, \n",
        "                        model, \n",
        "                        base_model, \n",
        "                        optimizer, \n",
        "                        loss, \n",
        "                        checkpointer_savepath)\n",
        "            \n",
        "  print (\"\\nInitializing training with  class labels: \" + \n",
        "         str(labels))\n",
        "  \n",
        "  model_summary_ = args.model_summary[0]\n",
        "  \n",
        "  if model_summary_ == True:\n",
        "      print (model.summary())\n",
        "  else:\n",
        "      print (\"\\nSuccessfully loaded deep neural network classifier for training ...\")\n",
        "      print (\"\\nReady, Steady, Go ...\")\n",
        "      print (\"\\n\")\n",
        "        \n",
        "  if not os.path.exists(os.path.join(args.output_dir[0] + '/checkpoint/')):\n",
        "    os.makedirs(os.path.join(args.output_dir[0] + '/checkpoint/'))\n",
        "    \n",
        "  lr = args.learning_rate[0]\n",
        "    \n",
        "  earlystopper = EarlyStopping(patience=6, \n",
        "                               verbose=1)\n",
        "  checkpointer = ModelCheckpoint(checkpointer_savepath, \n",
        "                                 verbose=1,  \n",
        "                                 save_best_only=True)\n",
        "  learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
        "                                              patience=2,\n",
        "                                              mode = 'max',\n",
        "                                              epsilon=1e-4, \n",
        "                                              cooldown=1,\n",
        "                                              verbose=1, \n",
        "                                              factor=0.5, \n",
        "                                              min_lr=lr*1e-2)\n",
        "  \n",
        "  model_train = model.fit_generator(train_generator,\n",
        "                                    epochs=nb_epoch,\n",
        "                                    steps_per_epoch=nb_train_samples//20,\n",
        "                                    validation_data=validation_generator,\n",
        "                                    validation_steps=nb_val_samples//20,\n",
        "                                    class_weight='auto', \n",
        "                                    callbacks=[earlystopper, \n",
        "                                               learning_rate_reduction, \n",
        "                                               checkpointer])\n",
        "  \n",
        "  if args.fine_tune[0] == True:\n",
        "      save_model(args, \"_ft_\", model)\n",
        "      generate_plot(args, \"_ft_\", model_train)\n",
        "  else:\n",
        "      save_model(args, \"_tl_\", model)\n",
        "      generate_plot(args, \"_tl_\", model_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ksm62NFGJm8s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "NB_FROZEN_LAYERS=170\n",
        "enable_training=True\n",
        "enable_dropout=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iZzNlWhb__tN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import types\n",
        "args=types.SimpleNamespace()\n",
        "args.base_model=['Inception_V4']\n",
        "args.frozen_layers=[NB_FROZEN_LAYERS]\n",
        "args.optimizer_val=['amsgrad']\n",
        "args.decay=[0.0]\n",
        "args.beta_2=[0.999]\n",
        "args.beta_1=[0.9]\n",
        "args.rho=[0.9]\n",
        "args.learning_rate=[1e-7]\n",
        "args.loss=['categorical_crossentropy']\n",
        "args.activation=['relu']\n",
        "args.epsilon=[1e-8]\n",
        "args.dropout=[0.4]\n",
        "args.test_hflip=[True]\n",
        "args.test_vflip=[True]\n",
        "args.test_zoom=[True]\n",
        "args.test_shear=[True]\n",
        "args.train_model=[enable_training]\n",
        "args.output_dir=['./{}/'.format(dataset_id)]\n",
        "args.root_dir=['./']\n",
        "args.val_dir=['./{}/validation/'.format(dataset_id)]\n",
        "args.train_dir=['./{}/train/'.format(dataset_id)]\n",
        "args.epoch=[20]\n",
        "args.batch=[10]\n",
        "args.train_aug=[True]\n",
        "args.test_aug=[True]\n",
        "args.normalize=[False]\n",
        "args.sub_sample=[False]\n",
        "args.load_weights=[False]\n",
        "args.fine_tune=[True]\n",
        "args.load_checkpoint=[True]\n",
        "args.model_summary=[False]\n",
        "args.plot=[True]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c__NUK3vTD7g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os \n",
        "import sys\n",
        "import subprocess\n",
        "import gc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KzfxBtXsTOQm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def execute_in_shell(command=None, \n",
        "                     verbose = False):\n",
        "    \"\"\" \n",
        "        command -- keyword argument, takes a list as input\n",
        "        verbsoe -- keyword argument, takes a boolean value as input\n",
        "    \n",
        "        This is a function that executes shell scripts from within python.\n",
        "        \n",
        "        Keyword argument 'command', should be a list of shell commands.\n",
        "        Keyword argument 'verbose', should be a boolean value to set verbose level.\n",
        "        \n",
        "        Example usage: execute_in_shell(command = ['ls ./some/folder/',\n",
        "                                                    ls ./some/folder/  -1 | wc -l'],\n",
        "                                        verbose = True ) \n",
        "                                        \n",
        "        This command returns dictionary with elements: Output and Error.\n",
        "        \n",
        "        Output records the console output,\n",
        "        Error records the console error messages.\n",
        "                                        \n",
        "    \"\"\"\n",
        "    error = []\n",
        "    output = []\n",
        "    \n",
        "    if isinstance(command, list):\n",
        "        for i in range(len(command)):\n",
        "            try:\n",
        "                process = subprocess.Popen(command[i], shell=True, stdout=subprocess.PIPE)\n",
        "                process.wait()\n",
        "                out, err = process.communicate()\n",
        "                error.append(err)\n",
        "                output.append(out)\n",
        "                if verbose:\n",
        "                    print ('Success running shell command: {}'.format(command[i]))\n",
        "            except Exception as e:\n",
        "                print ('Failed running shell command: {}'.format(command[i]))\n",
        "                if verbose:\n",
        "                    print(type(e))\n",
        "                    print(e.args)\n",
        "                    print(e)\n",
        "                \n",
        "    else:\n",
        "        print ('The argument command takes a list input ...')\n",
        "    return {'Output': output, 'Error': error }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T2goKEmHTg-e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1cb92ccf-5d6d-4141-e664-9eacca9094fb"
      },
      "cell_type": "code",
      "source": [
        "if setup:\n",
        "  execute_in_shell(command = command, \n",
        "                   verbose = True)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success running shell command: pip3 install -q kaggle PyDrive scikit-optimize >/dev/null 2>&1\n",
            "Success running shell command: mkdir /content/\n",
            "Success running shell command: mkdir /content/.kaggle/\n",
            "Success running shell command: mkdir ./kaggle_diabetic_retinopathy/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "54tRKt7_T4wB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "80bc11d4-194b-47e5-9f0b-8801f2d4016a"
      },
      "cell_type": "code",
      "source": [
        "IM_WIDTH, IM_HEIGHT = 299, 299                                                  # Default input image size for Inception v3 and v4 architecture\n",
        "DEFAULT_EPOCHS = 100\n",
        "DEFAULT_BATCHES = 20\n",
        "FC_SIZE = 4096\n",
        "DEFAULT_DROPOUT = 0.1\n",
        "DEFAULT_NB_LAYERS_TO_FREEZE = 169\n",
        "\n",
        "verbose = False\n",
        "\n",
        "sgd = SGD(lr=1e-7, decay=0.5, momentum=1, nesterov=True)\n",
        "rms = RMSprop(lr=1e-7, rho=0.9, epsilon=1e-08, decay=0.0)\n",
        "ada = Adagrad(lr=1e-3, epsilon=1e-08, decay=0.0)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8lJuRFJekEcE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "89552993-3f43-4b7f-adb6-a105fcfb6bc5"
      },
      "cell_type": "code",
      "source": [
        "command = ['rm -r ./{}/train/train/'.format(dataset_id)]\n",
        "execute_in_shell(command=command)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Error': [None], 'Output': [b'']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "metadata": {
        "id": "bam_VS-hU6HE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1465
        },
        "outputId": "cd9b22e4-4448-4f2b-ce72-fd0c4f1cd3c8"
      },
      "cell_type": "code",
      "source": [
        "if enable_training:\n",
        "  train(args)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using AmsGrad variant of Adam as the optimizer ...\n",
            "Optimizer parameters (recommended default): \n",
            "\n",
            " lr=1e-07 (0.001),                 \n",
            " beta_1=0.9 (0.9),               \n",
            " beta_2=0.999 (0.999),             \n",
            " epsilon=1e-08 (1e-08),             \n",
            " decay=0.0 (0.0)\n",
            "\n",
            "Total number of training samples = 34126\n",
            "Number of training classes = 5\n",
            "\n",
            "Total number of validation samples = 1000\n",
            "Number of validation classes = 5\n",
            "\n",
            "Initiating training session ...\n",
            "\n",
            "Failed to load custom training image augmentation parameters ...\n",
            "Loaded pre-set defaults ...\n",
            "To switch off image augmentation during training, set --train_augmentation flag to False\n",
            "\n",
            "Created image augmentation pipeline for training images ...\n",
            "Image augmentation parameters for training images:           \n",
            " image rotation range = 30,          \n",
            " width shift range = 0.2,          \n",
            " height shift range = 0.2,           \n",
            " shear range = 0.2 ,          \n",
            " zoom range = 0.2,           \n",
            " enable vertical flip = True,           \n",
            " enable horizontal flip = True\n",
            "\n",
            "Failed to load custom training image augmentation parameters ...\n",
            "Loaded pre-set defaults ...\n",
            "To switch off image augmentation during training, set --train_augmentation flag to False\n",
            "\n",
            "Created image augmentation pipeline for training images ...\n",
            "\n",
            "Image augmentation parameters for training images:\n",
            "\n",
            " image rotation range = 30,              \n",
            " width shift range = 0.2,              \n",
            " height shift range = 0.2,               \n",
            " shear range = 0.2 ,              \n",
            " zoom range = 0.2,               \n",
            " enable vertical flip = True,               \n",
            " enable horizontal flip = True\n",
            "\n",
            "Training labels: ['0', '1', '2', '3', '4']\n",
            "\n",
            "Validation labels: ['0', '1', '2', '3', '4']\n",
            "\n",
            "Generating training data: ... \n",
            "Found 34126 images belonging to 5 classes.\n",
            "\n",
            "Generating validation data: ... \n",
            "Found 1000 images belonging to 5 classes.\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.7/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "219062272/219055592 [==============================] - 3s 0us/step\n",
            "\n",
            "Base model: Inception version 4\n",
            "Building model using activation function: relu\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "New top layer added to: Inception version 4\n",
            "\n",
            "Tabula rasa ...\n",
            "Loaded default model weights ...\n",
            "\n",
            "Fine tuning Inception architecture ...\n",
            "Frozen layers: 170\n",
            "\n",
            "Initializing training with  class labels: ['0', '1', '2', '3', '4']\n",
            "\n",
            "Successfully loaded deep neural network classifier for training ...\n",
            "\n",
            "Ready, Steady, Go ...\n",
            "\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/20\n",
            " 159/1706 [=>............................] - ETA: 1:09:30 - loss: 1.6218 - acc: 0.2358"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lnF8VD08VGe9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}